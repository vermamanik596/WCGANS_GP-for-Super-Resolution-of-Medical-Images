{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1uKaHZcqeCa",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9a18aba-318d-4813-db13-c61399279cd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Collecting keras==3.5.0\n",
            "  Downloading keras-3.5.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras==3.5.0) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras==3.5.0) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras==3.5.0) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras==3.5.0) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras==3.5.0) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras==3.5.0) (0.14.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras==3.5.0) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras==3.5.0) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras==3.5.0) (4.13.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras==3.5.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras==3.5.0) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras==3.5.0) (0.1.2)\n",
            "Downloading keras-3.5.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.9.1\n",
            "    Uninstalling keras-3.9.1:\n",
            "      Successfully uninstalled keras-3.9.1\n",
            "Successfully installed keras-3.5.0\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "#!pip install keras==2.3.1\n",
        "!pip install tensorflow\n",
        "!pip install keras==3.5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6RjEfyVqhOk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4633101b-fe87-4044-ab6d-78d00e7f5617"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcocndnXqqOw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "from keras import Input\n",
        "from keras.applications import VGG19\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.layers import BatchNormalization, Activation, LeakyReLU, Add, Dense, Flatten\n",
        "from keras.layers import Conv2D, UpSampling2D\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.layers import PReLU\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, PReLU, add, Lambda, Reshape, Permute, Dot, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import UpSampling2D, LeakyReLU, Conv2D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import Input, Conv2D, LeakyReLU, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "\n",
        "import random\n",
        "from numpy import asarray\n",
        "from itertools import repeat\n",
        "\n",
        "import imageio\n",
        "from imageio import imread\n",
        "from PIL import Image\n",
        "from skimage.transform import resize as imresize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbA6tLfHq_BC"
      },
      "outputs": [],
      "source": [
        "# Define paths\n",
        "TRAIN_PATH = '/content/drive/MyDrive/CPR/chest_xray/train/'\n",
        "VAL_PATH = '/content/drive/MyDrive/CPR/chest_xray/val/'\n",
        "TEST_PATH = '/content/drive/MyDrive/CPR/chest_xray/test/'\n",
        "data_path = TRAIN_PATH\n",
        "\n",
        "# Training parameters\n",
        "epochs = 1000\n",
        "batch_size = 16\n",
        "\n",
        "# Image shapes\n",
        "low_resolution_shape = (64, 64, 3)\n",
        "high_resolution_shape = (256, 256, 3)\n",
        "\n",
        "# Optimizer\n",
        "common_optimizer = Adam(0.0002, 0.5)\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED = 2020\n",
        "tf.random.set_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yig0LKBPrZWr"
      },
      "outputs": [],
      "source": [
        "print(os.getcwd())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evbIQ3yNraJY"
      },
      "outputs": [],
      "source": [
        "def get_train_images(data_path):\n",
        "    CLASSES = ['NORMAL', 'PNEUMONIA']\n",
        "    image_list = []\n",
        "\n",
        "    for class_type in CLASSES:\n",
        "        image_list.extend(glob.glob(data_path + class_type + '/*'))\n",
        "\n",
        "    return image_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGat2l2frf7N"
      },
      "outputs": [],
      "source": [
        "\n",
        "def find_img_dims(image_list):\n",
        "\n",
        "    min_size = []\n",
        "    max_size = []\n",
        "\n",
        "    for i in range(len(image_list)):\n",
        "        im = Image.open(image_list[i])\n",
        "        min_size.append(min(im.size))\n",
        "        max_size.append(max(im.size))\n",
        "\n",
        "    return min(min_size), max(max_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOKLKWvOrjOk"
      },
      "outputs": [],
      "source": [
        "# # get min/max image sizes\n",
        "\n",
        "image_list = get_train_images(data_path)\n",
        "# min_size, max_size = find_img_dims(image_list)\n",
        "# print('The min and max image dims are {} and {} respectively.'\n",
        "#       .format(min_size, max_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfizZaoDrlB-"
      },
      "outputs": [],
      "source": [
        "image_list = image_list[:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNcPH2S6rmvE"
      },
      "outputs": [],
      "source": [
        "len(image_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2hQU8m4rprE"
      },
      "outputs": [],
      "source": [
        "# image_list ## Cek dataset sebelum dan setelah diproses pengurangan resolusi\n",
        "# image_list = get_train_images(data_path)\n",
        "\n",
        "low_resolution_shape = (32, 32, 3)\n",
        "low_resolution_shape = (64, 64, 3)\n",
        "high_resolution_shape = (256, 256, 3)\n",
        "\n",
        "def sample_images(image_list, batch_size, high_resolution_shape, low_resolution_shape):\n",
        "\n",
        "    \"\"\"\n",
        "    Pre-process a batch of training images\n",
        "    \"\"\"\n",
        "\n",
        "    # image_list is the list of all images\n",
        "    # ransom sample a batch of images\n",
        "    images_batch = np.random.choice(image_list, size=batch_size)\n",
        "\n",
        "    lr_images = []\n",
        "    hr_images = []\n",
        "\n",
        "\n",
        "    for img in images_batch:\n",
        "\n",
        "        img1 = imread(img, pilmode='RGB')\n",
        "        #img1 = imread(img, pilmode='RGB')\n",
        "        img1 = img1.astype(np.float32)\n",
        "\n",
        "        # change the size\n",
        "        img1_high_resolution = imresize(img1, high_resolution_shape)\n",
        "        img1_low_resolution = imresize(img1, low_resolution_shape)\n",
        "\n",
        "\n",
        "        # do a random horizontal flip\n",
        "        if np.random.random() < 0.5:\n",
        "            img1_high_resolution = np.fliplr(img1_high_resolution)\n",
        "            img1_low_resolution = np.fliplr(img1_low_resolution)\n",
        "\n",
        "        hr_images.append(img1_high_resolution)\n",
        "        lr_images.append(img1_low_resolution)\n",
        "\n",
        "\n",
        "    # convert lists into numpy ndarrays\n",
        "    return np.array(hr_images), np.array(lr_images)\n",
        "\n",
        "\n",
        "hr_images, lr_images = sample_images(image_list,\n",
        "                                     batch_size=batch_size,\n",
        "                                     low_resolution_shape=low_resolution_shape,\n",
        "                                     high_resolution_shape=high_resolution_shape)\n",
        "\n",
        "# normalize the images\n",
        "hr_images = hr_images / 127.5 - 1.\n",
        "lr_images = lr_images / 127.5 - 1.\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1,2, figsize=(10, 6))\n",
        "\n",
        "for idx in range(len(hr_images)):\n",
        "\n",
        "    titles = ['HR', 'LR']\n",
        "    images = [hr_images[idx], lr_images[idx]]\n",
        "\n",
        "    for i, img in enumerate(images):\n",
        "        ax[i].imshow((img + 1)/2.0, cmap='gray')\n",
        "        ax[i].axis(\"off\")\n",
        "        ax[i].set_title(titles[i])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKhul8P_rrsL"
      },
      "outputs": [],
      "source": [
        "def compute_psnr(original_image, generated_image):\n",
        "\n",
        "    original_image = tf.convert_to_tensor(original_image, dtype=tf.float32)\n",
        "    generated_image = tf.convert_to_tensor(generated_image, dtype=tf.float32)\n",
        "    psnr = tf.image.psnr(original_image, generated_image, max_val=1.0)\n",
        "\n",
        "    return tf.math.reduce_mean(psnr, axis=None, keepdims=False, name=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hShraD3fJ9kZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d6eS0NprvfF"
      },
      "outputs": [],
      "source": [
        "def plot_psnr(psnr):\n",
        "\n",
        "    psnr_means = psnr['psnr_quality']\n",
        "    plt.figure(figsize=(10,8))\n",
        "    plt.plot(psnr_means)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('PSNR')\n",
        "    plt.title('PSNR')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWpUPCn2rzjx"
      },
      "source": [
        "**SSIM - Structural Similarity Index**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgTSVnLer0TP"
      },
      "outputs": [],
      "source": [
        "def compute_ssim(original_image, generated_image):\n",
        "\n",
        "    original_image = tf.convert_to_tensor(original_image, dtype=tf.float32)\n",
        "    generated_image = tf.convert_to_tensor(generated_image, dtype=tf.float32)\n",
        "    ssim = tf.image.ssim(original_image, generated_image, max_val=1.0, filter_size=11,\n",
        "                          filter_sigma=1.5, k1=0.01, k2=0.03)\n",
        "\n",
        "    return tf.math.reduce_mean(ssim, axis=None, keepdims=False, name=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyKp7DQXr3xU"
      },
      "outputs": [],
      "source": [
        "def plot_ssim(ssim):\n",
        "\n",
        "    ssim_means = ssim['ssim_quality']\n",
        "\n",
        "    plt.figure(figsize=(10,8))\n",
        "    plt.plot(ssim_means)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('SSIM')\n",
        "    plt.title('SSIM')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXWJPtmyr96G"
      },
      "source": [
        "**Plot loss function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLqRqATnr5H5"
      },
      "outputs": [],
      "source": [
        "def plot_loss(losses):\n",
        "\n",
        "    d_loss = losses['d_history']\n",
        "    g_loss = losses['g_history']\n",
        "\n",
        "    plt.figure(figsize=(10,8))\n",
        "    plt.plot(d_loss, label=\"Discriminator loss\")\n",
        "    plt.plot(g_loss, label=\"Generator loss\")\n",
        "\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(\"Loss\")\n",
        "    plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Wc50ERPsB62"
      },
      "outputs": [],
      "source": [
        "def sample_images(image_list, batch_size, high_resolution_shape, low_resolution_shape):\n",
        "\n",
        "    \"\"\"\n",
        "    Pre-process a batch of training images\n",
        "    \"\"\"\n",
        "\n",
        "    # image_list is the list of all images\n",
        "    # ransom sample a batch of images\n",
        "    images_batch = np.random.choice(image_list, size=batch_size)\n",
        "\n",
        "    lr_images = []\n",
        "    hr_images = []\n",
        "\n",
        "\n",
        "    for img in images_batch:\n",
        "\n",
        "#         img1 = imread(img, mode=False, pilmode='RGB')\n",
        "        img1 = imread(img, pilmode='RGB')\n",
        "        img1 = img1.astype(np.float32)\n",
        "\n",
        "        # change the size\n",
        "        img1_high_resolution = imresize(img1, high_resolution_shape)\n",
        "        img1_low_resolution = imresize(img1, low_resolution_shape)\n",
        "\n",
        "\n",
        "        # do a random horizontal flip\n",
        "        if np.random.random() < 0.5:\n",
        "            img1_high_resolution = np.fliplr(img1_high_resolution)\n",
        "            img1_low_resolution = np.fliplr(img1_low_resolution)\n",
        "\n",
        "        hr_images.append(img1_high_resolution)\n",
        "        lr_images.append(img1_low_resolution)\n",
        "\n",
        "\n",
        "    # convert lists into numpy ndarrays\n",
        "    return np.array(hr_images), np.array(lr_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m271ZzcusFQ5"
      },
      "outputs": [],
      "source": [
        "def save_images(original_image, lr_image, sr_image, path):\n",
        "\n",
        "    \"\"\"\n",
        "    Save LR, HR (original) and generated SR\n",
        "    images in one panel\n",
        "    \"\"\"\n",
        "\n",
        "    fig, ax = plt.subplots(1,3, figsize=(10, 6))\n",
        "\n",
        "    images = [original_image, lr_image, sr_image]\n",
        "    titles = ['HR', 'LR','SR - generated']\n",
        "\n",
        "    for idx,img in enumerate(images):\n",
        "        # (X + 1)/2 to scale back from [-1,1] to [0,1]\n",
        "        ax[idx].imshow((img + 1)/2.0, cmap='gray')\n",
        "        ax[idx].axis(\"off\")\n",
        "    for idx, title in enumerate(titles):\n",
        "        ax[idx].set_title('{}'.format(title))\n",
        "\n",
        "    plt.savefig(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epq3oQeUsIQq"
      },
      "source": [
        "SRGAN - VGG19"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yqva-et45xMA"
      },
      "source": [
        "*Generator*  RUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgPCGzjE3EfV"
      },
      "outputs": [],
      "source": [
        "# Define upsampling block with L2 regularization\n",
        "def upsampling_block(x, filters, weight_decay=1e-5):\n",
        "    \"\"\"Upsampling block with L2 regularization\"\"\"\n",
        "    x = Conv2D(filters, kernel_size=3, strides=1, padding='same',\n",
        "               kernel_regularizer=tf.keras.regularizers.l2(weight_decay))(x)\n",
        "    x = UpSampling2D(size=2)(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    return x\n",
        "\n",
        "def residual_block(x, filters=64, kernel_size=3, strides=1, padding='same', momentum=0.8):\n",
        "    \"\"\"\n",
        "    Define a residual block for the generator network.\n",
        "    \"\"\"\n",
        "    res = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, kernel_regularizer=regularizers.l2(1e-4))(x)\n",
        "    res = Activation('relu')(res)\n",
        "    res = BatchNormalization(momentum=momentum)(res)\n",
        "    res = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, kernel_regularizer=regularizers.l2(1e-4))(res)\n",
        "    res = BatchNormalization(momentum=momentum)(res)\n",
        "    res = Add()([res, x])\n",
        "    return res\n",
        "\n",
        "def build_enhanced_generator():\n",
        "    # Define input shape\n",
        "    input_shape = (64, 64, 3)\n",
        "\n",
        "    # Input layer\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Pre-residual block\n",
        "    c1 = Conv2D(64, kernel_size=9, strides=1, padding='same')(inputs)\n",
        "    c1 = PReLU(shared_axes=[1, 2])(c1)\n",
        "\n",
        "    # Residual blocks\n",
        "    r = residual_block(c1, 64, 1)\n",
        "    for _ in range(15):  # 16 residual blocks in total\n",
        "        r = residual_block(r, 64, 1)\n",
        "\n",
        "    # Post-residual block\n",
        "    c2 = Conv2D(64, kernel_size=3, strides=1, padding='same')(r)\n",
        "    c2 = BatchNormalization()(c2)\n",
        "    c2 = add([c1, c2])\n",
        "\n",
        "    # Add self-attention mechanism\n",
        "    c2 = self_attention_block(c2, 64)\n",
        "\n",
        "    # Upsampling blocks\n",
        "    u1 = upsampling_block(c2, 256)\n",
        "    u2 = upsampling_block(u1, 256)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = Conv2D(3, kernel_size=9, strides=1, padding='same', activation='tanh')(u2)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs=inputs, outputs=outputs, name='Generator')\n",
        "    return model\n",
        "\n",
        "# Self-attention block as a Keras layer\n",
        "def self_attention_block(x, channels):\n",
        "    # Query, key, value transformations\n",
        "    f = Conv2D(channels // 8, kernel_size=1, strides=1, padding='same')(x)\n",
        "    g = Conv2D(channels // 8, kernel_size=1, strides=1, padding='same')(x)\n",
        "    h = Conv2D(channels, kernel_size=1, strides=1, padding='same')(x)\n",
        "\n",
        "    # Get shape information\n",
        "    shape = K.int_shape(f)\n",
        "    height, width = shape[1], shape[2]\n",
        "    num_channels = shape[3]\n",
        "\n",
        "    # Reshape using Keras operations with correct dimensions\n",
        "    f_flat = Reshape((height * width, num_channels))(f)\n",
        "    g_flat = Reshape((height * width, num_channels))(g)\n",
        "    h_flat = Reshape((height * width, channels))(h)\n",
        "\n",
        "    # Transpose g_flat for matrix multiplication\n",
        "    g_flat_transposed = Permute((2, 1))(g_flat)\n",
        "\n",
        "    # Calculate attention map\n",
        "    s = Dot(axes=(2, 1))([f_flat, g_flat_transposed])\n",
        "    beta = Activation('softmax')(s)\n",
        "\n",
        "    # Apply attention\n",
        "    o = Dot(axes=(2, 1))([beta, h_flat])\n",
        "    o = Reshape((height, width, channels))(o)\n",
        "\n",
        "    # Scale and add residual connection\n",
        "    gamma = 0.1  # Fixed scaling factor\n",
        "    o = Lambda(lambda x: x * gamma)(o)\n",
        "    x = add([x, o])\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjUes3HHDWjD"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Flatten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaGiIuHOt-Sc"
      },
      "outputs": [],
      "source": [
        "# Spectral Normalization for Discriminator\n",
        "def spectral_norm(w, iterations=1):\n",
        "    \"\"\"\n",
        "    Spectral normalization for weights to improve training stability\n",
        "    \"\"\"\n",
        "    w_shape = w.shape.as_list()\n",
        "    w = tf.reshape(w, [-1, w_shape[-1]])\n",
        "\n",
        "    u = tf.Variable(tf.random.normal([1, w_shape[-1]]), trainable=False)\n",
        "\n",
        "    u_hat = u\n",
        "    v_hat = None\n",
        "    for i in range(iterations):\n",
        "        # Power iteration\n",
        "        v_ = tf.matmul(u_hat, tf.transpose(w))\n",
        "        v_hat = tf.nn.l2_normalize(v_)\n",
        "\n",
        "        u_ = tf.matmul(v_hat, w)\n",
        "        u_hat = tf.nn.l2_normalize(u_)\n",
        "\n",
        "    sigma = tf.matmul(tf.matmul(v_hat, w), tf.transpose(u_hat))\n",
        "    w_norm = w / sigma\n",
        "\n",
        "    return tf.reshape(w_norm, w_shape)\n",
        "\n",
        "def build_enhanced_discriminator():\n",
        "    # define hyperparameters\n",
        "    leakyrelu_alpha = 0.2\n",
        "    momentum = 0.7\n",
        "\n",
        "    # the input is the HR shape\n",
        "    input_shape = (256, 256, 3)\n",
        "\n",
        "    # input layer for discriminator\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    # 8 convolutional layers with batch normalization and spectral normalization\n",
        "    dis1 = Conv2D(filters=64, kernel_size=3, strides=1, padding='same',\n",
        "                 kernel_initializer=tf.keras.initializers.GlorotUniform())(input_layer)\n",
        "    dis1 = LeakyReLU(alpha=leakyrelu_alpha)(dis1)\n",
        "\n",
        "    dis2 = Conv2D(filters=64, kernel_size=3, strides=2, padding='same',\n",
        "                 kernel_initializer=tf.keras.initializers.GlorotUniform())(dis1)\n",
        "    dis2 = LeakyReLU(alpha=leakyrelu_alpha)(dis2)\n",
        "    dis2 = BatchNormalization(momentum=momentum)(dis2)\n",
        "\n",
        "    dis3 = Conv2D(filters=128, kernel_size=3, strides=1, padding='same')(dis2)\n",
        "    dis3 = LeakyReLU(alpha=leakyrelu_alpha)(dis3)\n",
        "    dis3 = BatchNormalization(momentum=momentum)(dis3)\n",
        "\n",
        "    dis4 = Conv2D(filters=128, kernel_size=3, strides=2, padding='same')(dis3)\n",
        "    dis4 = LeakyReLU(alpha=leakyrelu_alpha)(dis4)\n",
        "    dis4 = BatchNormalization(momentum=0.8)(dis4)\n",
        "\n",
        "    dis5 = Conv2D(256, kernel_size=3, strides=1, padding='same')(dis4)\n",
        "    dis5 = LeakyReLU(alpha=leakyrelu_alpha)(dis5)\n",
        "    dis5 = BatchNormalization(momentum=momentum)(dis5)\n",
        "\n",
        "    dis6 = Conv2D(filters=256, kernel_size=3, strides=2, padding='same')(dis5)\n",
        "    dis6 = LeakyReLU(alpha=leakyrelu_alpha)(dis6)\n",
        "    dis6 = BatchNormalization(momentum=momentum)(dis6)\n",
        "\n",
        "    dis7 = Conv2D(filters=512, kernel_size=3, strides=1, padding='same')(dis6)\n",
        "    dis7 = LeakyReLU(alpha=leakyrelu_alpha)(dis7)\n",
        "    dis7 = BatchNormalization(momentum=momentum)(dis7)\n",
        "\n",
        "    dis8 = Conv2D(filters=512, kernel_size=3, strides=2, padding='same')(dis7)\n",
        "    dis8 = LeakyReLU(alpha=leakyrelu_alpha)(dis8)\n",
        "    dis8 = BatchNormalization(momentum=momentum)(dis8)\n",
        "\n",
        "     # Add flatten layer before dense layers\n",
        "    dis8_flat = Flatten()(dis8)\n",
        "\n",
        "    # fully connected layer\n",
        "    dis9 = Dense(units=1024)(dis8_flat)\n",
        "    dis9 = LeakyReLU(alpha=0.2)(dis9)\n",
        "\n",
        "\n",
        "    # WGAN modification: Remove sigmoid activation for Wasserstein loss\n",
        "    output = Dense(units=1, kernel_regularizer=regularizers.l2(1e-5))(dis9)\n",
        "\n",
        "    model = Model(inputs=[input_layer], outputs=[output], name='discriminator')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjQTlFx5-RQI"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aW5a1po6uGFa"
      },
      "outputs": [],
      "source": [
        "def build_VGG19():\n",
        "    # Load the pre-trained VGG19 model without the top (fully connected) layers\n",
        "    VGG19_base = VGG19(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
        "\n",
        "    # Freeze the layers of the pre-trained model\n",
        "    for layer in VGG19_base.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Create a new model with the VGG19 base and a custom top\n",
        "    input_layer = Input(shape=(256, 256, 3))\n",
        "    features = VGG19_base(input_layer)\n",
        "    model = Model(inputs=input_layer, outputs=features)\n",
        "\n",
        "    return model\n",
        "\n",
        "def build_adversarial_model(generator, discriminator, feature_extractor):\n",
        "    # input layer for high-resolution images\n",
        "    input_high_resolution = Input(shape=high_resolution_shape)\n",
        "\n",
        "    # input layer for low-resolution images\n",
        "    input_low_resolution = Input(shape=low_resolution_shape)\n",
        "\n",
        "    # generate high-resolution images from low-resolution images\n",
        "    generated_high_resolution_images = generator(input_low_resolution)\n",
        "\n",
        "    # Resize the generated images to match the expected input shape of VGG19\n",
        "    resized_images = layers.Lambda(lambda x: tf.image.resize(x, (256, 256)))(generated_high_resolution_images)\n",
        "\n",
        "    # extract feature maps from resized generated images\n",
        "    features = feature_extractor(resized_images)\n",
        "\n",
        "    # make the discriminator non-trainable\n",
        "    discriminator.trainable = False\n",
        "\n",
        "    # discriminator will give us a probability estimation for the generated high-resolution images\n",
        "    probs = discriminator(generated_high_resolution_images)\n",
        "\n",
        "    # create and compile\n",
        "    adversarial_model = Model([input_low_resolution, input_high_resolution], [probs, features])\n",
        "\n",
        "    # WGAN modification: Use MSE for both losses since we're not using binary cross-entropy anymore\n",
        "    adversarial_model.compile(loss=['mse', 'mse'], loss_weights=[1e-3, 1], optimizer=Adam(0.0002, 0.5))\n",
        "\n",
        "    return adversarial_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lW8JYZ5ouLru"
      },
      "outputs": [],
      "source": [
        "# Wasserstein loss function 1\n",
        "def wasserstein_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(y_true * y_pred)\n",
        "\n",
        "# Gradient Penalty Implementation\n",
        "def gradient_penalty(discriminator, real_samples, fake_samples, batch_size=16):\n",
        "    \"\"\"\n",
        "    Calculates the gradient penalty for WGAN-GP\n",
        "    \"\"\"\n",
        "    # Get random interpolation between real and fake samples\n",
        "    alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n",
        "    diff = fake_samples - real_samples\n",
        "    interpolated = real_samples + alpha * diff\n",
        "\n",
        "    with tf.GradientTape() as gp_tape:\n",
        "        gp_tape.watch(interpolated)\n",
        "        # Get the discriminator output for interpolated images\n",
        "        pred = discriminator(interpolated, training=True)\n",
        "\n",
        "    # Calculate gradients with respect to inputs\n",
        "    grads = gp_tape.gradient(pred, interpolated)\n",
        "    # Calculate the norm of the gradients\n",
        "    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
        "    # Calculate the gradient penalty\n",
        "    gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
        "\n",
        "    return gp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5sXw5Ys11O1"
      },
      "outputs": [],
      "source": [
        "# Wasserstein loss function 2\n",
        "def wasserstein_loss(y_true, y_pred):\n",
        "    # Reshape y_pred if it has spatial dimensions\n",
        "    #if len(K.int_shape(y_pred)) > 2:\n",
        "        #y_pred = Flatten()(y_pred)\n",
        "\n",
        "    # Ensure y_true has the same shape as y_pred\n",
        "    #y_true = K.reshape(y_true, K.shape(y_pred))\n",
        "\n",
        "    return K.mean(y_true * y_pred)\n",
        "\n",
        "def gradient_penalty(discriminator, real_samples, fake_samples, batch_size=8):\n",
        "    # Create a custom model for gradient penalty calculation\n",
        "    real_input = tf.keras.layers.Input(shape=real_samples.shape[1:])\n",
        "    fake_input = tf.keras.layers.Input(shape=fake_samples.shape[1:])\n",
        "\n",
        "    # Random weighted average\n",
        "    alpha = tf.random.uniform(shape=[batch_size, 1, 1, 1], minval=0., maxval=1.)\n",
        "    interpolated = real_input + alpha * (fake_input - real_input)\n",
        "\n",
        "    # Get discriminator output for interpolated images\n",
        "    interpolated_output = discriminator(interpolated)\n",
        "\n",
        "    # Create a model that takes real and fake samples and outputs gradient penalty\n",
        "    gp_model = tf.keras.models.Model([real_input, fake_input], interpolated_output)\n",
        "\n",
        "    # Use gradient tape to calculate gradients\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(interpolated)\n",
        "        interpolated_output = gp_model([real_samples, fake_samples])\n",
        "\n",
        "    # Calculate gradients with respect to the interpolated samples\n",
        "    gradients = tape.gradient(interpolated_output, interpolated)\n",
        "\n",
        "    # Compute the gradient norm\n",
        "    gradients_norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1, 2, 3]))\n",
        "\n",
        "    # Return the gradient penalty: (norm - 1)²\n",
        "    gradient_penalty = tf.reduce_mean(tf.square(gradients_norm - 1.0))\n",
        "    return gradient_penalty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbb6ZFSCBk8c"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzqvE2xQwDbf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize as imresize\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8K4aYitBjcB"
      },
      "outputs": [],
      "source": [
        "# Fix the gradient penalty calculation in the train_wgan_gp function\n",
        "def train_wgan_gp(generator, discriminator, feature_extractor, adversarial_model,\n",
        "                 data_path, epochs=1000, batch_size=16,\n",
        "                 low_resolution_shape=(64, 64, 3),\n",
        "                 high_resolution_shape=(256, 256, 3),\n",
        "                 lambda_gp=10, n_critic=7,\n",
        "                 sample_interval=50,\n",
        "                 checkpoint_interval=50):\n",
        "    \"\"\"Train WGAN-GP model\"\"\"\n",
        "    # Create output directory\n",
        "    output_dir = os.path.join(os.path.dirname(data_path), \"outputs\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Get training images\n",
        "    train_images = get_train_images(data_path)\n",
        "    print(f\"Found {len(train_images)} training images\")\n",
        "\n",
        "    # If no images found, exit\n",
        "    if len(train_images) == 0:\n",
        "        print(\"No training images found!\")\n",
        "        return None, None, None, None, None\n",
        "\n",
        "    # Initialize optimizers\n",
        "    d_optimizer = Adam(0.0001, beta_1=0.5, beta_2=0.9)\n",
        "\n",
        "    # Compile discriminator\n",
        "    discriminator.compile(\n",
        "        loss=wasserstein_loss,\n",
        "        optimizer=d_optimizer\n",
        "    )\n",
        "\n",
        "    # Initialize arrays to store metrics\n",
        "    d_losses = []\n",
        "    g_losses = []\n",
        "    psnr_values = []\n",
        "    ssim_values = []\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Initialize epoch metrics\n",
        "        epoch_d_losses = []\n",
        "        epoch_g_losses = []\n",
        "        epoch_psnr = []\n",
        "        epoch_ssim = []\n",
        "\n",
        "        # Randomly select batch_size images\n",
        "        idx = np.random.randint(0, len(train_images), batch_size)\n",
        "\n",
        "        # Load and preprocess images\n",
        "        hr_images = []\n",
        "        lr_images = []\n",
        "\n",
        "        for i in idx:\n",
        "            img = imread(train_images[i])\n",
        "            img = img.astype(np.float32)\n",
        "\n",
        "            # Resize to high and low resolution\n",
        "            hr_img = imresize(img, high_resolution_shape)\n",
        "            lr_img = imresize(img, low_resolution_shape)\n",
        "\n",
        "            # Normalize to [-1, 1]\n",
        "            hr_img = hr_img / 127.5 - 1.\n",
        "            lr_img = lr_img / 127.5 - 1.\n",
        "\n",
        "            hr_images.append(hr_img)\n",
        "            lr_images.append(lr_img)\n",
        "\n",
        "        hr_images = np.array(hr_images)\n",
        "        lr_images = np.array(lr_images)\n",
        "\n",
        "        # Generate fake images\n",
        "        generated_high_resolution_images = generator.predict(lr_images)\n",
        "\n",
        "        # Train discriminator\n",
        "        for _ in range(n_critic):\n",
        "            # Labels for Wasserstein loss\n",
        "            valid = -np.ones((batch_size, 1))  # Want to maximize D(real)\n",
        "            fake = np.ones((batch_size, 1))    # Want to minimize D(fake)\n",
        "\n",
        "            # Train on real images\n",
        "            d_loss_real = discriminator.train_on_batch(hr_images, valid)\n",
        "\n",
        "            # Train on fake images\n",
        "            d_loss_fake = discriminator.train_on_batch(generated_high_resolution_images, fake)\n",
        "\n",
        "            # Calculate gradient penalty\n",
        "            epsilon = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n",
        "            interpolated = hr_images + epsilon * (generated_high_resolution_images - hr_images)\n",
        "\n",
        "            with tf.GradientTape() as gp_tape:\n",
        "                gp_tape.watch(interpolated)\n",
        "                # Get the discriminator output for interpolated images\n",
        "                pred = discriminator(interpolated, training=True)\n",
        "\n",
        "            # Calculate gradients with respect to inputs\n",
        "            grads = gp_tape.gradient(pred, interpolated)\n",
        "            # Calculate the norm of the gradients\n",
        "            norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
        "            # Calculate the gradient penalty\n",
        "            gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
        "\n",
        "            # Total discriminator loss\n",
        "            d_loss = d_loss_real - d_loss_fake + lambda_gp * gp\n",
        "            epoch_d_losses.append(d_loss)\n",
        "\n",
        "        # Train generator\n",
        "        image_features = feature_extractor.predict(hr_images)\n",
        "        g_loss = adversarial_model.train_on_batch([lr_images, hr_images], [valid, image_features])\n",
        "        epoch_g_losses.append(g_loss[0])\n",
        "\n",
        "        # Calculate PSNR and SSIM\n",
        "        for i in range(batch_size):\n",
        "            # Convert to [0, 1] for metric calculation\n",
        "            hr = (hr_images[i] + 1) / 2.0\n",
        "            sr = (generated_high_resolution_images[i] + 1) / 2.0\n",
        "\n",
        "            # Calculate metrics\n",
        "            psnr_value = psnr(hr, sr, data_range=1.0)\n",
        "            ssim_value = ssim(hr, sr, data_range=1.0, win_size=5, channel_axis=2)\n",
        "\n",
        "            epoch_psnr.append(psnr_value)\n",
        "            epoch_ssim.append(ssim_value)\n",
        "\n",
        "        # Append epoch metrics\n",
        "        d_losses.append(np.mean(epoch_d_losses))\n",
        "        g_losses.append(np.mean(epoch_g_losses))\n",
        "        psnr_values.append(np.mean(epoch_psnr))\n",
        "        ssim_values.append(np.mean(epoch_ssim))\n",
        "\n",
        "        # Print progress\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - {elapsed_time:.2f}s - \"\n",
        "              f\"d_loss: {d_losses[-1]:.4f} - g_loss: {g_losses[-1]:.4f} - \"\n",
        "              f\"PSNR: {psnr_values[-1]:.4f} - SSIM: {ssim_values[-1]:.4f}\")\n",
        "\n",
        "        # Save sample images at specified intervals\n",
        "        if (epoch + 1) % sample_interval == 0:\n",
        "            # Generate and save images\n",
        "            for index, img in enumerate(generated_high_resolution_images):\n",
        "                save_images(hr_images[index], lr_images[index], img,\n",
        "                          path=f\"{output_dir}/img_{epoch+1}_{index}\")\n",
        "\n",
        "        # Save model checkpoints\n",
        "        if (epoch + 1) % checkpoint_interval == 0:  # checkpoint_interval will be defined\n",
        "            checkpoint_dir = os.path.join(output_dir, \"checkpoints\")\n",
        "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "            # Save generator model\n",
        "            generator.save(os.path.join(checkpoint_dir, f\"generator_epoch_{epoch+1}.keras\"))\n",
        "\n",
        "            # Save discriminator model\n",
        "            discriminator.save(os.path.join(checkpoint_dir, f\"discriminator_epoch_{epoch+1}.keras\"))\n",
        "\n",
        "            print(f\"Checkpoint saved at epoch {epoch+1}\")\n",
        "\n",
        "    # Compile metrics\n",
        "    metrics = {\n",
        "        'd_history': d_losses,\n",
        "        'g_history': g_losses,\n",
        "        'psnr_quality': psnr_values,\n",
        "        'ssim_quality': ssim_values\n",
        "    }\n",
        "    # Create plots directory\n",
        "    plots_dir = os.path.join(output_dir, \"plots\")\n",
        "    os.makedirs(plots_dir, exist_ok=True)\n",
        "\n",
        "    # Plot each metric individually\n",
        "    for metric_name, values in metrics.items():\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(range(len(values)), values, color='#1f77b4', linewidth=1.0)\n",
        "        plt.title(metric_name.upper(), fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('Epochs', fontsize=12)\n",
        "        plt.ylabel(metric_name.upper(), fontsize=12)\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(plots_dir, f\"{metric_name}.png\"), dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "    return generator, discriminator, metrics, psnr_values, ssim_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAiNk7XxuZRh"
      },
      "outputs": [],
      "source": [
        "# Build models\n",
        "generator = build_enhanced_generator()\n",
        "discriminator = build_enhanced_discriminator()\n",
        "feature_extractor = build_VGG19()\n",
        "\n",
        "# Compile feature extractor\n",
        "feature_extractor.trainable = False\n",
        "feature_extractor.compile(loss='mse', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
        "\n",
        "# Build adversarial model\n",
        "adversarial_model = build_adversarial_model(generator, discriminator, feature_extractor)\n",
        "\n",
        "# Train the model\n",
        "generator, discriminator, losses, psnr_values, ssim_values = train_wgan_gp(\n",
        "    generator, discriminator, feature_extractor, adversarial_model,\n",
        "    data_path=TRAIN_PATH, epochs=1000, batch_size=16,\n",
        "    low_resolution_shape=low_resolution_shape,\n",
        "    high_resolution_shape=high_resolution_shape,\n",
        "    lambda_gp=10, n_critic=7\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a46w30mZK8Px"
      },
      "outputs": [],
      "source": [
        "def visualize_results(generator, discriminator, metrics, test_images=None, num_samples=5):\n",
        "    \"\"\"\n",
        "    Visualize training results and generate sample images\n",
        "\n",
        "    Args:\n",
        "        generator: Trained generator model\n",
        "        discriminator: Trained discriminator model\n",
        "        metrics: Dictionary containing training metrics\n",
        "        test_images: List of test image paths (optional)\n",
        "        num_samples: Number of samples to generate\n",
        "    \"\"\"\n",
        "    # Create figure for training metrics\n",
        "    plt.figure(figsize=(20, 10))\n",
        "\n",
        "    # Plot discriminator and generator losses\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(metrics['d_history'], label='Discriminator Loss')\n",
        "    plt.plot(metrics['g_history'], label='Generator Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training Losses')\n",
        "\n",
        "    # Plot PSNR values\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(metrics['psnr_quality'])\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('PSNR (dB)')\n",
        "    plt.title('Peak Signal-to-Noise Ratio')\n",
        "\n",
        "    # Plot SSIM values\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(metrics['ssim_quality'])\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('SSIM')\n",
        "    plt.title('Structural Similarity Index')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_metrics.png')\n",
        "    plt.show()\n",
        "\n",
        "    # Generate and visualize sample images if test images are provided\n",
        "    if test_images is not None and len(test_images) > 0:\n",
        "        # Select random test images\n",
        "        indices = np.random.randint(0, len(test_images), num_samples)\n",
        "\n",
        "        # Create figure for sample images\n",
        "        plt.figure(figsize=(15, 5 * num_samples))\n",
        "\n",
        "        for i, idx in enumerate(indices):\n",
        "            # Load and preprocess image\n",
        "            img = imread(test_images[idx])\n",
        "            img = img.astype(np.float32)\n",
        "\n",
        "            # Create low-resolution version\n",
        "            lr_img = imresize(img, (64, 64, 3))\n",
        "            hr_img = imresize(img, (256, 256, 3))\n",
        "\n",
        "            # Normalize to [-1, 1]\n",
        "            lr_input = lr_img / 127.5 - 1.\n",
        "            hr_input = hr_img / 127.5 - 1.\n",
        "\n",
        "            # Generate super-resolution image\n",
        "            sr_img = generator.predict(np.expand_dims(lr_input, axis=0))[0]\n",
        "\n",
        "            # Convert back to [0, 255] for display\n",
        "            lr_display = (lr_input + 1) * 127.5\n",
        "            hr_display = (hr_input + 1) * 127.5\n",
        "            sr_display = (sr_img + 1) * 127.5\n",
        "\n",
        "            # Clip values to valid range\n",
        "            lr_display = np.clip(lr_display, 0, 255).astype(np.uint8)\n",
        "            hr_display = np.clip(hr_display, 0, 255).astype(np.uint8)\n",
        "            sr_display = np.clip(sr_display, 0, 255).astype(np.uint8)\n",
        "\n",
        "            # Display images\n",
        "            plt.subplot(num_samples, 3, i*3 + 1)\n",
        "            plt.imshow(lr_display)\n",
        "            plt.title(f'Low Resolution')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(num_samples, 3, i*3 + 2)\n",
        "            plt.imshow(sr_display)\n",
        "            plt.title(f'Generated (PSNR: {psnr(hr_display/255, sr_display/255, data_range=1.0):.2f}dB)')\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(num_samples, 3, i*3 + 3)\n",
        "            plt.imshow(hr_display)\n",
        "            plt.title('Original High Resolution')\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('sample_results.png')\n",
        "        plt.show()\n",
        "\n",
        "# Usage example after training\n",
        "# visualize_results(generator, discriminator, losses, test_images=get_test_images(TEST_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0XoUUv7LMtJ"
      },
      "outputs": [],
      "source": [
        "# Get test images\n",
        "test_images = get_train_images(TEST_PATH)  # or use a separate test set\n",
        "\n",
        "# Visualize results\n",
        "visualize_results(generator, discriminator, losses, test_images=test_images)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FCn7F8z4HvLv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}